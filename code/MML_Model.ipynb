{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "gpuClass": "premium",
      "collapsed_sections": [
        "Cz-W0PFH8KvD",
        "g8SCkw3l7Sg3",
        "L0Yhn9T564a9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Setting Things Up**\n",
        "\n",
        "**1 If you haven't already, please hit :**\n",
        "\n",
        "`File > Save a Copy in Drive`\n",
        "\n",
        "**to copy this notebook to your Google drive, and work on a copy. If you don't do this, your changes won't be saved!**\n",
        "\n",
        "\n",
        "**2 In order to use a GPU with your notebook, select :**\n",
        "\n",
        "`Runtime > Change runtime type`\n",
        "\n",
        "**menu, and then set the hardware accelerator dropdown to GPU. This can significantly speed up the training process.**\n",
        "\n",
        "**3 In order to have enough memory with your notebook, select :**\n",
        "\n",
        "`Runtime > Change runtime type`\n",
        "\n",
        "**menu, and then select High-RAM in the Runtime shape dropdown.**\n",
        "\n",
        "To facilitate your initial progress, we have included a ready-to-use code on Google Colab for this problem. It allows you to get started immediately. Additionally, if you prefer not to use Google Colab and prefer setting up your own programming environment or employing alternative methods, the provided files and code will still be valuable.\n",
        "\n",
        "**PS:You need manually install the `tensorflow_text` and `tf-models-official` libraries**\n",
        "\n",
        "\n",
        "**PS:You also need manually load pretrained Bert model weights `bert_en_uncased_preprocess_3` and `small_bert_bert_en_uncased_L-4_H-512_A-8_2` into `model weight` folder. The Bert model weights can be found in our datasets.**"
      ],
      "metadata": {
        "id": "UwLE3eyhNd5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "lvdTPQfzmnAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DwlxLjxm5fG",
        "outputId": "d727ab47-267d-400f-f1cc-018f18b2d1c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "id": "63nNfCb2OYUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-models-official"
      ],
      "metadata": {
        "id": "429PQ6QIOYvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*-coding:utf8 -*-\n",
        "import tensorflow as tf\n",
        "# print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib.pyplot import figure\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from tensorflow.python import keras\n",
        "from keras.layers import Dense, Flatten, Conv2D\n",
        "from keras.optimizers import RMSprop, Adam, SGD\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras import Model, Input, layers\n",
        "\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "from keras import Model, Input, layers, regularizers\n",
        "from keras.models import load_model\n",
        "from keras import activations"
      ],
      "metadata": {
        "id": "r9f3jnrspEse",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10cec740-0a27-4f5b-f5b4-9f53d4b467dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "UjHBjkEcphmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/Vehicle Rating Prediction/new_images_with_folder\n",
        "!unzip \"new_images_with_folder.zip\""
      ],
      "metadata": {
        "id": "k9joUMUZMlvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/Vehicle Rating Prediction/new_interior_images_with_folder\n",
        "!unzip \"new_interior_images_with_folder.zip\""
      ],
      "metadata": {
        "id": "tjSMpWOdMmKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# change to your personal project address\n",
        "\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/Vehicle Rating Prediction"
      ],
      "metadata": {
        "id": "YmCHZQggMo40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1 Data Processing**"
      ],
      "metadata": {
        "id": "kF2Apvy2bWlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var = \"total score\"\n",
        "# var = \"safety score\"\n",
        "# var = \"performance score\"\n",
        "# var = \"interior score\"\n",
        "# var = \"critics score\""
      ],
      "metadata": {
        "id": "iusFNApSMRqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 first we need to get the total score information from the csv file\n",
        "# read info_data\n",
        "file_name = \"parametric data 2571 normalize \" + var + \".csv\"\n",
        "info_data = pd.read_csv(file_name)\n",
        "# get numpy matrix which only contains data (do not contain the title)\n",
        "info_data = np.array(info_data)\n",
        "print(info_data.shape)  # (2571, 303)\n",
        "print(len(info_data))\n",
        "print(info_data.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i5-YuskSHwX",
        "outputId": "068546c6-26a1-4785-b389-df746cd71524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2571, 310)\n",
            "2571\n",
            "310\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input Parametric Data (2571x303)\n",
        "# column 0: origin index\n",
        "# column 1: model name\n",
        "# column 2-303: parametric feature\n",
        "# column 304: total score\n",
        "# column 305: critics score\n",
        "# column 306: performance score\n",
        "# column 307: interior score\n",
        "# column 308: safety score\n",
        "# column 309: data split index => 1: train data; 2: validation data; 3:test data"
      ],
      "metadata": {
        "id": "ZjSaAM3h4G4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train data shuffle index\n",
        "num1 = 2055\n",
        "idx1 = tf.range(num1)\n",
        "idx1 = tf.random.shuffle(idx1)\n",
        "# print(idx1)\n",
        "# print(idx1[0])\n",
        "with tf.compat.v1.Session():\n",
        "    index1 = idx1.numpy()\n",
        "# print(index1.shape)\n",
        "# print(index1[0])\n",
        "\n",
        "# validation data shuffle index\n",
        "num2 = 258\n",
        "idx2 = tf.range(num2)\n",
        "idx2 = tf.random.shuffle(idx2)\n",
        "# print(idx2)\n",
        "# print(idx2[0])\n",
        "with tf.compat.v1.Session():\n",
        "    index2 = idx2.numpy()\n",
        "# print(index2.shape)\n",
        "# print(index2[0])\n",
        "\n",
        "# test data shuffle index\n",
        "num3 = 258\n",
        "idx3 = tf.range(num3)\n",
        "idx3 = tf.random.shuffle(idx3)\n",
        "# print(idx3)\n",
        "# print(idx3[0])\n",
        "with tf.compat.v1.Session():\n",
        "    index3 = idx3.numpy()\n",
        "# print(index3.shape)\n",
        "# print(index3[0])"
      ],
      "metadata": {
        "id": "vomcoWMgSHqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assign the parametric data**"
      ],
      "metadata": {
        "id": "vOWT1FSAWC09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# assign the parametric data\n",
        "# To predict the total score, you will need to assign the y variable using the values in column 304 of the parametric data.\n",
        "# column 304: total score\n",
        "# column 305: critics score\n",
        "# column 306: performance score\n",
        "# column 307: interior score\n",
        "# column 308: safety score\n",
        "\n",
        "x_train_tab = np.zeros((num1, 302))\n",
        "y_train_tab = np.zeros((num1, 1))\n",
        "for i in range(num1):\n",
        "    x_train_tab[i, :] = np.array(info_data[index1[i], 2:304], dtype=float)\n",
        "    y_train_tab[i] = np.array(info_data[index1[i], 304], dtype=float)\n",
        "x_train_tab = tf.convert_to_tensor(x_train_tab)\n",
        "y_train_tab = tf.convert_to_tensor(y_train_tab)\n",
        "y_train = y_train_tab\n",
        "print(y_train.shape)\n",
        "\n",
        "x_validation_tab = np.zeros((num2, 302))\n",
        "y_validation_tab = np.zeros((num2, 1))\n",
        "for i in range(num2):\n",
        "    x_validation_tab[i, :] = np.array(info_data[index2[i] + num1, 2:304], dtype=float)\n",
        "    y_validation_tab[i] = np.array(info_data[index2[i] + num1, 304], dtype=float)\n",
        "x_validation_tab = tf.convert_to_tensor(x_validation_tab)\n",
        "y_validation_tab = tf.convert_to_tensor(y_validation_tab)\n",
        "y_validation = y_validation_tab\n",
        "print(y_validation.shape)\n",
        "\n",
        "x_test_tab = np.zeros((num3, 302))\n",
        "y_test_tab = np.zeros((num3, 1))\n",
        "for i in range(num3):\n",
        "    x_test_tab[i, :] = np.array(info_data[index3[i] + num1 + num2, 2:304], dtype=float)\n",
        "    y_test_tab[i] = np.array(info_data[index3[i] + num1 + num2, 304], dtype=float)\n",
        "x_test_tab = tf.convert_to_tensor(x_test_tab)\n",
        "y_test_tab = tf.convert_to_tensor(y_test_tab)\n",
        "y_test = y_test_tab\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "PI_ael2qSHjP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0e72ed0-c6ad-4a05-9f1a-d021ee59ca88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2055, 1)\n",
            "(258, 1)\n",
            "(258, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assign text data**"
      ],
      "metadata": {
        "id": "1jFKodwIV4f-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the text data\n",
        "var_name = 'data split ' + var\n",
        "\n",
        "sketch1 = pd.read_csv('text_data.csv', encoding='latin1')\n",
        "train_df = sketch1[sketch1[var_name] == 1]\n",
        "val_df = sketch1[sketch1[var_name] == 2]\n",
        "test_df = sketch1[sketch1[var_name] == 3]\n",
        "# print(train_df.shape)\n",
        "# print(train_df)\n",
        "\n",
        "sketch2 = train_df.astype({\"text\": str})\n",
        "text1 = list(sketch2['text'])\n",
        "\n",
        "sketch3 = val_df.astype({\"text\": str})\n",
        "text2 = list(sketch3['text'])\n",
        "\n",
        "sketch4 = test_df.astype({\"text\": str})\n",
        "text3 = list(sketch4['text'])\n",
        "\n",
        "\n",
        "train_text = [text1[i] for i in index1]\n",
        "x_train_text = tf.constant(train_text)\n",
        "\n",
        "validation_text = [text2[i] for i in index2]\n",
        "x_validation_text = tf.constant(validation_text)\n",
        "\n",
        "test_text = [text3[i] for i in index3]\n",
        "x_test_text = tf.constant(test_text)\n",
        "# print(len(train_text))\n",
        "# print(train_text[0])\n"
      ],
      "metadata": {
        "id": "fS-71IIvV3iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assign image data**"
      ],
      "metadata": {
        "id": "gsheil0ZOPgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please take note of the image dimensions. For interior images, each image has dimensions of 300x448x3, whereas for exterior images, each image has dimensions of 290x448x3.**"
      ],
      "metadata": {
        "id": "azYyabFWCEtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# assign the image data\n",
        "# exterior image => 290    interior image => 300\n",
        "image = np.zeros((290, 448, 3))\n",
        "x_train = np.zeros((num1, 290, 448, 3))\n",
        "x_validation = np.zeros((num2, 290, 448, 3))\n",
        "x_test = np.zeros((num3, 290, 448, 3))\n",
        "# image = image_data[0]\n",
        "\n",
        "# train data\n",
        "for i in range(num1):\n",
        "    folder_path = r'new_images_with_folder/' + info_data[index1[i]][1]\n",
        "    dirs = os.listdir(folder_path)\n",
        "    print(i)\n",
        "    if len(dirs) > 0:\n",
        "        # only one total picture\n",
        "        dirpath = folder_path + '/' + dirs[0]  # get the angular front view of the car\n",
        "        img = Image.open(dirpath)\n",
        "        img_plt = np.array(img)\n",
        "        x_train[i, :, :, :] = img_plt / 255.0\n",
        "\n",
        "\n",
        "# validation data\n",
        "for i in range(num2):\n",
        "    folder_path = r'new_images_with_folder/' + info_data[index2[i]+num1][1]\n",
        "    dirs = os.listdir(folder_path)\n",
        "    print(i)\n",
        "    if len(dirs) > 0:\n",
        "        # only one total picture\n",
        "        dirpath = folder_path + '/' + dirs[0]  # get the angular front view of the car\n",
        "        img = Image.open(dirpath)\n",
        "        img_plt = np.array(img)\n",
        "        x_validation[i, :, :, :] = img_plt / 255.0\n",
        "\n",
        "\n",
        "# test data\n",
        "for i in range(num3):\n",
        "    folder_path = r'new_images_with_folder/' + info_data[index3[i]+num1+num2][1]\n",
        "    dirs = os.listdir(folder_path)\n",
        "    print(i)\n",
        "    if len(dirs) > 0:\n",
        "        # only one total picture\n",
        "        dirpath = folder_path + '/' + dirs[0]  # get the angular front view of the car\n",
        "        img = Image.open(dirpath)\n",
        "        img_plt = np.array(img)\n",
        "        x_test[i, :, :, :] = img_plt / 255.0\n",
        "\n",
        "\n",
        "x_train_img = tf.convert_to_tensor(x_train)\n",
        "x_validation_img = tf.convert_to_tensor(x_validation)\n",
        "x_test_img = tf.convert_to_tensor(x_test)"
      ],
      "metadata": {
        "id": "tFsQNCYcORhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2-1 MML Model (Par+Text+Img)**"
      ],
      "metadata": {
        "id": "KLZ1dWMEbo5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct MML model\n",
        "adam_optimizer = Adam(learning_rate=0.00005)\n",
        "rms_prop_optimizer = RMSprop(learning_rate=0.001)\n",
        "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9, nesterov=False)"
      ],
      "metadata": {
        "id": "86oUekIs1L52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the pretrain model\n",
        "# When predicting total score, critics score, performance score and safety score, we only use exterior image to train the CNN model.\n",
        "# When predicting interior score, we only use interior image to train the CNN model.\n",
        "\n",
        "\n",
        "# CNN\n",
        "CNNmodel = tf.keras.models.load_model('model weight/' + var + '_Ex_Img.h5')\n",
        "# total score, safety score, performance score, critics score => _Ex_Img\n",
        "# interior socre => _In_Img\n",
        "for layer in CNNmodel.layers:\n",
        "  layer._name = layer._name + \"_a\"\n",
        "CNNmodel.summary()\n",
        "\n",
        "# MLP\n",
        "MLPmodel = tf.keras.models.load_model('model weight/' + var + '_Par.h5')\n",
        "for layer in MLPmodel.layers:\n",
        "  layer._name = layer._name + \"_b\"\n",
        "MLPmodel.summary()\n",
        "\n",
        "# Bert\n",
        "model_name = 'model weight/' + var + '_Text.h5'\n",
        "Bertmodel = tf.keras.models.load_model(model_name, custom_objects={'KerasLayer': hub.KerasLayer})\n",
        "for layer in Bertmodel.layers:\n",
        "  layer._name = layer._name + \"_c\"\n",
        "Bertmodel.summary()\n"
      ],
      "metadata": {
        "id": "nHiUrQ6nZr21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in CNNmodel.layers:\n",
        "    layer.trainable = True\n",
        "    print(layer.name, layer)\n",
        "\n",
        "CNN_weight = CNNmodel.layers[-1].get_weights()[0]\n",
        "CNN_bias = CNNmodel.layers[-1].get_weights()[1]\n",
        "print(CNN_weight)\n",
        "print(CNN_bias)\n",
        "\n",
        "for layer in MLPmodel.layers:\n",
        "    layer.trainable = True\n",
        "    print(layer.name, layer)\n",
        "MLP_weight = MLPmodel.layers[-1].get_weights()[0]\n",
        "MLP_bias = MLPmodel.layers[-1].get_weights()[1]\n",
        "print(MLP_weight)\n",
        "print(MLP_bias)\n",
        "\n",
        "for layer in Bertmodel.layers:\n",
        "    layer.trainable = True\n",
        "    print(layer.name, layer)\n",
        "Bert_weight = Bertmodel.layers[-1].get_weights()[0]\n",
        "Bert_bias = Bertmodel.layers[-1].get_weights()[1]\n",
        "print(Bert_weight)\n",
        "print(Bert_bias)\n",
        "\n",
        "# These coefficients are calculated from the file --- \"Get Linear Regression Weights\"\n",
        "initializer1 = []\n",
        "for i in range(100):\n",
        "    initializer1.append((MLP_weight[i] * 0.92594368))\n",
        "for i in range(100):\n",
        "    initializer1.append((Bert_weight[i] * 0.00564145))\n",
        "for i in range(100):\n",
        "    initializer1.append((CNN_weight[i] * 0.17254082))\n",
        "initializer1 = tf.keras.initializers.Constant(initializer1)\n",
        "\n",
        "print(initializer1)\n",
        "print('finished')\n",
        "\n",
        "out1_img = CNNmodel.layers[-2].output\n",
        "out1_par = MLPmodel.layers[-2].output\n",
        "out1_text = Bertmodel.layers[-2].output\n",
        "out2 = tf.keras.layers.Concatenate(axis=1, name='concatenation_tab_text_img')([out1_par, out1_text, out1_img])\n",
        "out5 = layers.Dense(1, activation='relu', name='concatenation_dense', kernel_initializer=initializer1)(out2)\n",
        "model = Model([MLPmodel.input, Bertmodel.input, CNNmodel.input], out5, name='MML_Model_Par_Text_Img')\n",
        "\n",
        "model.compile(\n",
        "    optimizer=adam_optimizer,\n",
        "    loss='mse',\n",
        "    metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mse', 'mae']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "OU8-k5OrHYd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scheduler(epoch, lr):\n",
        "    min_lr = 0.0000001\n",
        "    if epoch < 2:\n",
        "        return lr\n",
        "    else:\n",
        "        if lr < min_lr:\n",
        "            lr = min_lr\n",
        "            return lr\n",
        "        else:\n",
        "          return lr * tf.math.exp(-0.01)\n",
        "          # return lr\n"
      ],
      "metadata": {
        "id": "OFItpnon9LxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(patience=10, monitor=\"val_loss\", restore_best_weights=True, verbose=1)\n",
        "EPOCHS = 200\n",
        "history = model.fit([x_train_tab, x_train_text, x_train_img], y_train, epochs=EPOCHS, batch_size=32, validation_data=([x_validation_tab, x_validation_text, x_validation_img], y_validation), verbose=2, callbacks=[early_stop, reduce_lr])\n",
        "print(history)"
      ],
      "metadata": {
        "id": "C3WaMiWv-HkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_rmse, test_mse, test_mae = model.evaluate([x_test_tab, x_test_text, x_test_img], y_test, verbose=2)\n",
        "validation_loss, validation_rmse, validation_mse, validation_mae = model.evaluate([x_validation_tab, x_validation_text, x_validation_img], y_validation, verbose=2)\n",
        "train_loss, train_rmse, train_mse, train_mae = model.evaluate([x_train_tab, x_train_text, x_train_img], y_train, verbose=2)"
      ],
      "metadata": {
        "id": "o_DLTDmp_ExS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9363232d-22e3-4f91-d831-9fe04ddcd265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 - 0s - loss: 0.0042 - rmse: 0.0646 - mse: 0.0042 - mae: 0.0496 - 496ms/epoch - 55ms/step\n",
            "9/9 - 0s - loss: 0.0046 - rmse: 0.0677 - mse: 0.0046 - mae: 0.0501 - 494ms/epoch - 55ms/step\n",
            "65/65 - 4s - loss: 1.5234e-04 - rmse: 0.0123 - mse: 1.5234e-04 - mae: 0.0096 - 4s/epoch - 55ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# store the model\n",
        "# summarize the loaded model\n",
        "model.summary()\n",
        "# save the best performing model to file\n",
        "model_name = 'model weight/' + var + '_MML_Par_Text_Img.h5'\n",
        "model.save_weights(model_name)\n",
        "# model.save('model weight/' + var + '_MML_Par_Text_Img.h5', 'saved_model') # infeasible"
      ],
      "metadata": {
        "id": "-YSikL87-ixk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.predict([x_test_tab, x_test_text, x_test_img])\n",
        "print(result)"
      ],
      "metadata": {
        "id": "46Gi48Z5_OVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(4, 3), dpi=80)\n",
        "plt.scatter(y_test, result, s=3)\n",
        "x = [0, 1]\n",
        "y = [0, 1]\n",
        "plt.plot(x, y, color=\"black\")\n",
        "plt.xlabel(var + ' ground truth')\n",
        "plt.ylabel(var + ' prediction')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "4Rjujb7s-wsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import nan\n",
        "test = np.array(y_test).T\n",
        "# print(test)\n",
        "predict = np.array(result).T\n",
        "\n",
        "correlation_matrix = np.corrcoef(test, predict)\n",
        "print(correlation_matrix)\n",
        "correlation_xy = correlation_matrix[0,1]\n",
        "r_squared = correlation_xy**2\n",
        "##range: 0.8199158648859902\n",
        "\n",
        "print (r_squared)"
      ],
      "metadata": {
        "id": "tog5DrHoFA6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pd.DataFrame()\n",
        "idx = 0\n",
        "output.loc[idx, 'train_rmse'] = train_rmse\n",
        "output.loc[idx, 'train_mse'] = train_mse\n",
        "output.loc[idx, 'train_mae'] = train_mae\n",
        "\n",
        "output.loc[idx, 'validation_rmse'] = validation_rmse\n",
        "output.loc[idx, 'validation_mse'] = validation_mse\n",
        "output.loc[idx, 'validation_mae'] = validation_mae\n",
        "\n",
        "output.loc[idx, 'test_rmse'] = test_rmse\n",
        "output.loc[idx, 'test_mse'] = test_mse\n",
        "output.loc[idx, 'test_mae'] = test_mae\n",
        "\n",
        "output.loc[idx, 'r^2'] = r_squared\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(output)\n",
        "\n",
        "output.to_csv('MML_value.csv')"
      ],
      "metadata": {
        "id": "XVFQUny8emGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2-2 MML Model (Text+Img)**"
      ],
      "metadata": {
        "id": "Cz-W0PFH8KvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct MML model\n",
        "adam_optimizer = Adam(learning_rate=0.00005)\n",
        "rms_prop_optimizer = RMSprop(learning_rate=0.001)\n",
        "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9, nesterov=False)"
      ],
      "metadata": {
        "id": "fk3fJemp8KvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the pretrain model\n",
        "# When predicting total score, critics score, performance score and safety score, we only use exterior image to train the CNN model.\n",
        "# When predicting interior score, we only use interior image to train the CNN model.\n",
        "\n",
        "\n",
        "# CNN\n",
        "CNNmodel = tf.keras.models.load_model('model weight/' + var + '_Ex_Img.h5')\n",
        "# interior socre => _In_Img\n",
        "for layer in CNNmodel.layers:\n",
        "  layer._name = layer._name + \"_a\"\n",
        "CNNmodel.summary()\n",
        "\n",
        "# Bert\n",
        "model_name = 'model weight/' + var + '_Text.h5'\n",
        "Bertmodel = tf.keras.models.load_model(model_name, custom_objects={'KerasLayer': hub.KerasLayer})\n",
        "for layer in Bertmodel.layers:\n",
        "  layer._name = layer._name + \"_c\"\n",
        "Bertmodel.summary()\n"
      ],
      "metadata": {
        "id": "uGAMP7jl8KvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in CNNmodel.layers:\n",
        "    layer.trainable = True\n",
        "    print(layer.name, layer)\n",
        "\n",
        "CNN_weight = CNNmodel.layers[-1].get_weights()[0]\n",
        "CNN_bias = CNNmodel.layers[-1].get_weights()[1]\n",
        "print(CNN_weight)\n",
        "print(CNN_bias)\n",
        "\n",
        "for layer in Bertmodel.layers:\n",
        "    layer.trainable = True\n",
        "    print(layer.name, layer)\n",
        "Bert_weight = Bertmodel.layers[-1].get_weights()[0]\n",
        "Bert_bias = Bertmodel.layers[-1].get_weights()[1]\n",
        "print(Bert_weight)\n",
        "print(Bert_bias)\n",
        "\n",
        "# These coefficients are calculated from the file --- \"Get Linear Regression Weights\"\n",
        "initializer1 = []\n",
        "for i in range(100):\n",
        "    initializer1.append((Bert_weight[i] * 0.00564145))\n",
        "for i in range(100):\n",
        "    initializer1.append((CNN_weight[i] * 0.17254082))\n",
        "initializer1 = tf.keras.initializers.Constant(initializer1)\n",
        "\n",
        "print(initializer1)\n",
        "print('finished')\n",
        "\n",
        "out1_img = CNNmodel.layers[-2].output\n",
        "out1_text = Bertmodel.layers[-2].output\n",
        "out2 = tf.keras.layers.Concatenate(axis=1, name='concatenation_text_img')([out1_text, out1_img])\n",
        "out5 = layers.Dense(1, activation='relu', name='concatenation_dense', kernel_initializer=initializer1)(out2)\n",
        "model = Model([Bertmodel.input, CNNmodel.input], out5, name='MML_Model_Text_Img')\n",
        "\n",
        "model.compile(\n",
        "    optimizer=adam_optimizer,\n",
        "    loss='mse',\n",
        "    metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mse', 'mae']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "b2FFxuiI8KvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scheduler(epoch, lr):\n",
        "    min_lr = 0.0000001\n",
        "    if epoch < 2:\n",
        "        return lr\n",
        "    else:\n",
        "        if lr < min_lr:\n",
        "            lr = min_lr\n",
        "            return lr\n",
        "        else:\n",
        "          return lr * tf.math.exp(-0.01)\n",
        "          # return lr\n"
      ],
      "metadata": {
        "id": "9Z2o7EZP8KvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(patience=10, monitor=\"val_loss\", restore_best_weights=True, verbose=1)\n",
        "EPOCHS = 200\n",
        "history = model.fit([x_train_text, x_train_img], y_train, epochs=EPOCHS, batch_size=32, validation_data=([x_validation_text, x_validation_img], y_validation), verbose=2, callbacks=[early_stop, reduce_lr])\n",
        "print(history)"
      ],
      "metadata": {
        "id": "mFkIRGW48KvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_rmse, test_mse, test_mae = model.evaluate([x_test_text, x_test_img], y_test, verbose=2)\n",
        "validation_loss, validation_rmse, validation_mse, validation_mae = model.evaluate([x_validation_text, x_validation_img], y_validation, verbose=2)\n",
        "train_loss, train_rmse, train_mse, train_mae = model.evaluate([x_train_text, x_train_img], y_train, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9363232d-22e3-4f91-d831-9fe04ddcd265",
        "id": "QmUYfTIx8KvQ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 - 0s - loss: 0.0042 - rmse: 0.0646 - mse: 0.0042 - mae: 0.0496 - 496ms/epoch - 55ms/step\n",
            "9/9 - 0s - loss: 0.0046 - rmse: 0.0677 - mse: 0.0046 - mae: 0.0501 - 494ms/epoch - 55ms/step\n",
            "65/65 - 4s - loss: 1.5234e-04 - rmse: 0.0123 - mse: 1.5234e-04 - mae: 0.0096 - 4s/epoch - 55ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# store the model\n",
        "# summarize the loaded model\n",
        "model.summary()\n",
        "# save the best performing model to file\n",
        "model_name = 'model weight/' + var + '_MML_Text_Img.h5'\n",
        "model.save_weights(model_name)\n",
        "# model.save('model weight/' + var + '_MML_Text_Img.h5', 'saved_model') # infeasible"
      ],
      "metadata": {
        "id": "c9lLF_Y-8KvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.predict([x_test_text, x_test_img])\n",
        "print(result)"
      ],
      "metadata": {
        "id": "IZnxMTKs8KvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(4, 3), dpi=80)\n",
        "plt.scatter(y_test, result, s=3)\n",
        "x = [0, 1]\n",
        "y = [0, 1]\n",
        "plt.plot(x, y, color=\"black\")\n",
        "plt.xlabel(var + ' ground truth')\n",
        "plt.ylabel(var + ' prediction')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "-hRDfg8t8KvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import nan\n",
        "test = np.array(y_test).T\n",
        "# print(test)\n",
        "predict = np.array(result).T\n",
        "\n",
        "correlation_matrix = np.corrcoef(test, predict)\n",
        "print(correlation_matrix)\n",
        "correlation_xy = correlation_matrix[0,1]\n",
        "r_squared = correlation_xy**2\n",
        "##range: 0.8199158648859902\n",
        "\n",
        "print (r_squared)"
      ],
      "metadata": {
        "id": "BaZICJch8KvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pd.DataFrame()\n",
        "idx = 0\n",
        "output.loc[idx, 'train_rmse'] = train_rmse\n",
        "output.loc[idx, 'train_mse'] = train_mse\n",
        "output.loc[idx, 'train_mae'] = train_mae\n",
        "\n",
        "output.loc[idx, 'validation_rmse'] = validation_rmse\n",
        "output.loc[idx, 'validation_mse'] = validation_mse\n",
        "output.loc[idx, 'validation_mae'] = validation_mae\n",
        "\n",
        "output.loc[idx, 'test_rmse'] = test_rmse\n",
        "output.loc[idx, 'test_mse'] = test_mse\n",
        "output.loc[idx, 'test_mae'] = test_mae\n",
        "\n",
        "output.loc[idx, 'r^2'] = r_squared\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(output)\n",
        "\n",
        "output.to_csv('MML_value.csv')"
      ],
      "metadata": {
        "id": "hgTjE73H8KvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2-3 MML Model (Par+Img)**"
      ],
      "metadata": {
        "id": "g8SCkw3l7Sg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct MML model\n",
        "adam_optimizer = Adam(learning_rate=0.00005)\n",
        "rms_prop_optimizer = RMSprop(learning_rate=0.001)\n",
        "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9, nesterov=False)"
      ],
      "metadata": {
        "id": "4H-wN0RT7ShD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the pretrain model\n",
        "# When predicting total score, critics score, performance score and safety score, we only use exterior image to train the CNN model.\n",
        "# When predicting interior score, we only use interior image to train the CNN model.\n",
        "\n",
        "\n",
        "# CNN\n",
        "CNNmodel = tf.keras.models.load_model('model weight/' + var + '_Ex_Img.h5')\n",
        "# interior socre => _In_Img\n",
        "for layer in CNNmodel.layers:\n",
        "  layer._name = layer._name + \"_a\"\n",
        "CNNmodel.summary()\n",
        "\n",
        "# MLP\n",
        "MLPmodel = tf.keras.models.load_model('model weight/' + var + '_Par.h5')\n",
        "for layer in MLPmodel.layers:\n",
        "  layer._name = layer._name + \"_b\"\n",
        "MLPmodel.summary()"
      ],
      "metadata": {
        "id": "wsJe4-lL7ShD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in CNNmodel.layers:\n",
        "    layer.trainable = True\n",
        "    print(layer.name, layer)\n",
        "\n",
        "CNN_weight = CNNmodel.layers[-1].get_weights()[0]\n",
        "CNN_bias = CNNmodel.layers[-1].get_weights()[1]\n",
        "print(CNN_weight)\n",
        "print(CNN_bias)\n",
        "\n",
        "for layer in MLPmodel.layers:\n",
        "    layer.trainable = True\n",
        "    print(layer.name, layer)\n",
        "MLP_weight = MLPmodel.layers[-1].get_weights()[0]\n",
        "MLP_bias = MLPmodel.layers[-1].get_weights()[1]\n",
        "print(MLP_weight)\n",
        "print(MLP_bias)\n",
        "\n",
        "# These coefficients are calculated from the file --- \"Get Linear Regression Weights\"\n",
        "initializer1 = []\n",
        "for i in range(100):\n",
        "    initializer1.append((MLP_weight[i] * 0.92594368))\n",
        "for i in range(100):\n",
        "    initializer1.append((CNN_weight[i] * 0.17254082))\n",
        "initializer1 = tf.keras.initializers.Constant(initializer1)\n",
        "\n",
        "print(initializer1)\n",
        "print('finished')\n",
        "\n",
        "out1_img = CNNmodel.layers[-2].output\n",
        "out1_par = MLPmodel.layers[-2].output\n",
        "out2 = tf.keras.layers.Concatenate(axis=1)([out1_par, out1_img])\n",
        "out5 = layers.Dense(1, activation='relu', name='concatenation_dense', kernel_initializer=initializer1)(out2)\n",
        "model = Model([MLPmodel.input, CNNmodel.input], out5)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=adam_optimizer,\n",
        "    loss='mse',\n",
        "    metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mse', 'mae']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "gbPum96N7ShD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scheduler(epoch, lr):\n",
        "    min_lr = 0.0000001\n",
        "    if epoch < 2:\n",
        "        return lr\n",
        "    else:\n",
        "        if lr < min_lr:\n",
        "            lr = min_lr\n",
        "            return lr\n",
        "        else:\n",
        "          return lr * tf.math.exp(-0.01)\n",
        "          # return lr\n"
      ],
      "metadata": {
        "id": "Qi5teRlm7ShE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(patience=10, monitor=\"val_loss\", restore_best_weights=True, verbose=1)\n",
        "EPOCHS = 200\n",
        "history = model.fit([x_train_tab, x_train_img], y_train, epochs=EPOCHS, batch_size=32, validation_data=([x_validation_tab, x_validation_img], y_validation), verbose=2, callbacks=[early_stop, reduce_lr])\n",
        "print(history)"
      ],
      "metadata": {
        "id": "jrAlRkaQ7ShE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_rmse, test_mse, test_mae = model.evaluate([x_test_tab, x_test_img], y_test, verbose=2)\n",
        "validation_loss, validation_rmse, validation_mse, validation_mae = model.evaluate([x_validation_tab, x_validation_img], y_validation, verbose=2)\n",
        "train_loss, train_rmse, train_mse, train_mae = model.evaluate([x_train_tab, x_train_img], y_train, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9363232d-22e3-4f91-d831-9fe04ddcd265",
        "id": "DWsEHF9o7ShE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 - 0s - loss: 0.0042 - rmse: 0.0646 - mse: 0.0042 - mae: 0.0496 - 496ms/epoch - 55ms/step\n",
            "9/9 - 0s - loss: 0.0046 - rmse: 0.0677 - mse: 0.0046 - mae: 0.0501 - 494ms/epoch - 55ms/step\n",
            "65/65 - 4s - loss: 1.5234e-04 - rmse: 0.0123 - mse: 1.5234e-04 - mae: 0.0096 - 4s/epoch - 55ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# store the model\n",
        "# summarize the loaded model\n",
        "model.summary()\n",
        "# save the best performing model to file\n",
        "model_name = 'model weight/' + var + '_MML_Par_Img.h5'\n",
        "model.save_weights(model_name)\n",
        "# model.save('model weight/' + var + '_MML_Par_Text_Img.h5', 'saved_model') # infeasible"
      ],
      "metadata": {
        "id": "BLlrtHmW7ShF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.predict([x_test_tab, x_test_img])\n",
        "print(result)"
      ],
      "metadata": {
        "id": "ZgNKYv6-7ShF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(4, 3), dpi=80)\n",
        "plt.scatter(y_test, result, s=3)\n",
        "x = [0, 1]\n",
        "y = [0, 1]\n",
        "plt.plot(x, y, color=\"black\")\n",
        "plt.xlabel(var + ' ground truth')\n",
        "plt.ylabel(var + ' prediction')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "ZkNh_fbD7ShF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import nan\n",
        "test = np.array(y_test).T\n",
        "# print(test)\n",
        "predict = np.array(result).T\n",
        "\n",
        "correlation_matrix = np.corrcoef(test, predict)\n",
        "print(correlation_matrix)\n",
        "correlation_xy = correlation_matrix[0,1]\n",
        "r_squared = correlation_xy**2\n",
        "##range: 0.8199158648859902\n",
        "\n",
        "print (r_squared)"
      ],
      "metadata": {
        "id": "CKGgrYTY7ShF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pd.DataFrame()\n",
        "idx = 0\n",
        "output.loc[idx, 'train_rmse'] = train_rmse\n",
        "output.loc[idx, 'train_mse'] = train_mse\n",
        "output.loc[idx, 'train_mae'] = train_mae\n",
        "\n",
        "output.loc[idx, 'validation_rmse'] = validation_rmse\n",
        "output.loc[idx, 'validation_mse'] = validation_mse\n",
        "output.loc[idx, 'validation_mae'] = validation_mae\n",
        "\n",
        "output.loc[idx, 'test_rmse'] = test_rmse\n",
        "output.loc[idx, 'test_mse'] = test_mse\n",
        "output.loc[idx, 'test_mae'] = test_mae\n",
        "\n",
        "output.loc[idx, 'r^2'] = r_squared\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(output)\n",
        "\n",
        "output.to_csv('MML_value.csv')"
      ],
      "metadata": {
        "id": "NSjlEq1x7ShF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2-4 MML Model (Par+Text)**"
      ],
      "metadata": {
        "id": "L0Yhn9T564a9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# construct MML model\n",
        "adam_optimizer = Adam(learning_rate=0.00005)\n",
        "rms_prop_optimizer = RMSprop(learning_rate=0.001)\n",
        "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9, nesterov=False)"
      ],
      "metadata": {
        "id": "__VpJ7W69l6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the pretrain model\n",
        "# When predicting total score, critics score, performance score and safety score, we only use exterior image to train the CNN model.\n",
        "# When predicting interior score, we only use interior image to train the CNN model.\n",
        "\n",
        "\n",
        "# MLP\n",
        "MLPmodel = tf.keras.models.load_model('model weight/' + var + '_Par.h5')\n",
        "for layer in MLPmodel.layers:\n",
        "  layer._name = layer._name + \"_b\"\n",
        "MLPmodel.summary()\n",
        "\n",
        "# Bert\n",
        "model_name = 'model weight/' + var + '_Text.h5'\n",
        "Bertmodel = tf.keras.models.load_model(model_name, custom_objects={'KerasLayer': hub.KerasLayer})\n",
        "for layer in Bertmodel.layers:\n",
        "  layer._name = layer._name + \"_c\"\n",
        "Bertmodel.summary()"
      ],
      "metadata": {
        "id": "visHdHzU9p6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for layer in MLPmodel.layers:\n",
        "    layer.trainable = True\n",
        "    print(layer.name, layer)\n",
        "MLP_weight = MLPmodel.layers[-1].get_weights()[0]\n",
        "MLP_bias = MLPmodel.layers[-1].get_weights()[1]\n",
        "print(MLP_weight)\n",
        "print(MLP_bias)\n",
        "\n",
        "for layer in Bertmodel.layers:\n",
        "    layer.trainable = True\n",
        "    print(layer.name, layer)\n",
        "Bert_weight = Bertmodel.layers[-1].get_weights()[0]\n",
        "Bert_bias = Bertmodel.layers[-1].get_weights()[1]\n",
        "print(Bert_weight)\n",
        "print(Bert_bias)\n",
        "\n",
        "# These coefficients are calculated from the file --- \"Get Linear Regression Weights\"\n",
        "initializer1 = []\n",
        "for i in range(100):\n",
        "    initializer1.append((MLP_weight[i] * 0.92594368))\n",
        "for i in range(100):\n",
        "    initializer1.append((Bert_weight[i] * 0.00564145))\n",
        "initializer1 = tf.keras.initializers.Constant(initializer1)\n",
        "\n",
        "print(initializer1)\n",
        "print('finished')\n",
        "\n",
        "out1_par = MLPmodel.layers[-2].output\n",
        "out1_text = Bertmodel.layers[-2].output\n",
        "out2 = tf.keras.layers.Concatenate(axis=1)([out1_par, out1_text])\n",
        "out5 = layers.Dense(1, activation='relu', name='concatenation_dense', kernel_initializer=initializer1)(out2)\n",
        "model = Model([MLPmodel.input, Bertmodel.input], out5)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=adam_optimizer,\n",
        "    loss='mse',\n",
        "    metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mse', 'mae']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "Iwd-_4BB64bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scheduler(epoch, lr):\n",
        "    min_lr = 0.0000001\n",
        "    if epoch < 2:\n",
        "        return lr\n",
        "    else:\n",
        "        if lr < min_lr:\n",
        "            lr = min_lr\n",
        "            return lr\n",
        "        else:\n",
        "          return lr * tf.math.exp(-0.01)\n",
        "          # return lr"
      ],
      "metadata": {
        "id": "fmep1ucp64bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(patience=10, monitor=\"val_loss\", restore_best_weights=True, verbose=1)\n",
        "EPOCHS = 200\n",
        "history = model.fit([x_train_tab, x_train_text], y_train, epochs=EPOCHS, batch_size=32, validation_data=([x_validation_tab, x_validation_text], y_validation), verbose=2, callbacks=[early_stop, reduce_lr])\n",
        "print(history)"
      ],
      "metadata": {
        "id": "waaooKXT64bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_rmse, test_mse, test_mae = model.evaluate([x_test_tab, x_test_text], y_test, verbose=2)\n",
        "validation_loss, validation_rmse, validation_mse, validation_mae = model.evaluate([x_validation_tab, x_validation_text], y_validation, verbose=2)\n",
        "train_loss, train_rmse, train_mse, train_mae = model.evaluate([x_train_tab, x_train_text], y_train, verbose=2)"
      ],
      "metadata": {
        "id": "NZUonBeK64bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store the model\n",
        "# summarize the loaded model\n",
        "model.summary()\n",
        "# save the best performing model to file\n",
        "model_name = 'model weight/' + var + '_MML_Par_Text.h5'\n",
        "model.save_weights(model_name)\n",
        "# model.save('model weight/' + var + '_MML_Par_Text.h5', 'saved_model') # infeasible"
      ],
      "metadata": {
        "id": "KeLrAAiS64bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.predict([x_test_tab, x_test_text, x_test_img])\n",
        "print(result)"
      ],
      "metadata": {
        "id": "3SUAh2NU64bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(4, 3), dpi=80)\n",
        "plt.scatter(y_test, result, s=3)\n",
        "x = [0, 1]\n",
        "y = [0, 1]\n",
        "plt.plot(x, y, color=\"black\")\n",
        "plt.xlabel(var + ' ground truth')\n",
        "plt.ylabel(var + ' prediction')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "eXR9kjnp64bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import nan\n",
        "test = np.array(y_test).T\n",
        "# print(test)\n",
        "predict = np.array(result).T\n",
        "\n",
        "correlation_matrix = np.corrcoef(test, predict)\n",
        "print(correlation_matrix)\n",
        "correlation_xy = correlation_matrix[0,1]\n",
        "r_squared = correlation_xy**2\n",
        "##range: 0.8199158648859902\n",
        "\n",
        "print (r_squared)"
      ],
      "metadata": {
        "id": "yNSEXtR064bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = pd.DataFrame()\n",
        "idx = 0\n",
        "output.loc[idx, 'train_rmse'] = train_rmse\n",
        "output.loc[idx, 'train_mse'] = train_mse\n",
        "output.loc[idx, 'train_mae'] = train_mae\n",
        "\n",
        "output.loc[idx, 'validation_rmse'] = validation_rmse\n",
        "output.loc[idx, 'validation_mse'] = validation_mse\n",
        "output.loc[idx, 'validation_mae'] = validation_mae\n",
        "\n",
        "output.loc[idx, 'test_rmse'] = test_rmse\n",
        "output.loc[idx, 'test_mse'] = test_mse\n",
        "output.loc[idx, 'test_mae'] = test_mae\n",
        "\n",
        "output.loc[idx, 'r^2'] = r_squared\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(output)\n",
        "\n",
        "output.to_csv('MML_value.csv')"
      ],
      "metadata": {
        "id": "AcfT6P5Z64bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3 Verification: Load model again(we need construct structure again and load weights to verify our model was stored successfully)**"
      ],
      "metadata": {
        "id": "aXG0el2xZibV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 construct MML model\n",
        "adam_optimizer = Adam(learning_rate=0.00005)\n",
        "rms_prop_optimizer = RMSprop(learning_rate=0.001)\n",
        "sgd_optimizer = SGD(learning_rate=0.01, momentum=0.9, nesterov=False)\n",
        "\n",
        "# load the pretrain model\n",
        "# CNN\n",
        "CNNmodel = tf.keras.models.load_model('model weight/' + var + '_Ex_Img.h5')\n",
        "# interior socre => _In_Img\n",
        "for layer in CNNmodel.layers:\n",
        "  layer._name = layer._name + \"_a\"\n",
        "CNNmodel.summary()\n",
        "\n",
        "# MLP\n",
        "MLPmodel = tf.keras.models.load_model('model weight/' + var + '_Par.h5')\n",
        "for layer in MLPmodel.layers:\n",
        "  layer._name = layer._name + \"_b\"\n",
        "MLPmodel.summary()\n",
        "\n",
        "# Bert\n",
        "model_name = 'model weight/' + var + '_Text.h5'\n",
        "Bertmodel = tf.keras.models.load_model(model_name, custom_objects={'KerasLayer': hub.KerasLayer})\n",
        "for layer in Bertmodel.layers:\n",
        "  layer._name = layer._name + \"_c\"\n",
        "Bertmodel.summary()\n",
        "\n",
        "\n",
        "for layer in CNNmodel.layers:\n",
        "    layer.trainable = True\n",
        "    print(layer.name, layer)\n",
        "\n",
        "CNN_weight = CNNmodel.layers[-1].get_weights()[0]\n",
        "CNN_bias = CNNmodel.layers[-1].get_weights()[1]\n",
        "# print(CNN_weight)\n",
        "# print(CNN_bias)\n",
        "\n",
        "for layer in MLPmodel.layers:\n",
        "    layer.trainable = True\n",
        "    print(layer.name, layer)\n",
        "MLP_weight = MLPmodel.layers[-1].get_weights()[0]\n",
        "MLP_bias = MLPmodel.layers[-1].get_weights()[1]\n",
        "# print(MLP_weight)\n",
        "# print(MLP_bias)\n",
        "\n",
        "for layer in Bertmodel.layers:\n",
        "    layer.trainable = True\n",
        "    print(layer.name, layer)\n",
        "Bert_weight = Bertmodel.layers[-1].get_weights()[0]\n",
        "Bert_bias = Bertmodel.layers[-1].get_weights()[1]\n",
        "# print(Bert_weight)\n",
        "# print(Bert_bias)\n",
        "\n",
        "# These coefficients are calculated from the file --- \"Get Linear Regression Weights\"\n",
        "initializer1 = []\n",
        "for i in range(100):\n",
        "    initializer1.append((MLP_weight[i] * 0.92594368))\n",
        "for i in range(100):\n",
        "    initializer1.append((Bert_weight[i] * 0.00564145))\n",
        "for i in range(100):\n",
        "    initializer1.append((CNN_weight[i] * 0.17254082))\n",
        "initializer1 = tf.keras.initializers.Constant(initializer1)\n",
        "\n",
        "print(initializer1)\n",
        "print('finished')\n",
        "\n",
        "out1_img = CNNmodel.layers[-2].output\n",
        "out1_par = MLPmodel.layers[-2].output\n",
        "out1_text = Bertmodel.layers[-2].output\n",
        "out2 = tf.keras.layers.Concatenate(axis=1)([out1_par, out1_text, out1_img])\n",
        "out5 = layers.Dense(1, activation='relu', name='concatenation_dense', kernel_initializer=initializer1)(out2)\n",
        "model1 = Model([MLPmodel.input, Bertmodel.input, CNNmodel.input], out5)\n",
        "\n",
        "model1.compile(\n",
        "    optimizer=adam_optimizer,\n",
        "    loss='mse',\n",
        "    metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse'), 'mse', 'mae']\n",
        ")\n",
        "\n",
        "model1.summary()\n"
      ],
      "metadata": {
        "id": "zNAOv_B2Zqf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'model weight/' + var + '_MML_Par_Text_Img.h5'\n",
        "model1.load_weights(model_name)\n",
        "model1.summary()\n",
        "print('yes')"
      ],
      "metadata": {
        "id": "06sF7OLMYZD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result1 = model1.predict([x_test_tab, x_test_text, x_test_img])\n",
        "# print(result1)"
      ],
      "metadata": {
        "id": "X1X2WFhsgxTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure(figsize=(4, 3), dpi=400)\n",
        "plt.scatter(y_test, result1, s=3)\n",
        "x = [0, 1]\n",
        "y = [0, 1]\n",
        "plt.plot(x, y, color=\"black\")\n",
        "plt.xlabel(var + ' ground truth')\n",
        "plt.ylabel(var + ' prediction')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.tight_layout()\n",
        "# plt.savefig(\"MML_total_score.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fq3EEdBVhbhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = np.array(y_test).T\n",
        "# print(test)\n",
        "predict = np.array(result1).T\n",
        "\n",
        "correlation_matrix = np.corrcoef(test, predict)\n",
        "print(correlation_matrix)\n",
        "correlation_xy = correlation_matrix[0,1]\n",
        "r_squared = correlation_xy**2\n",
        "##range: 0.8199158648859902\n",
        "\n",
        "print (r_squared)"
      ],
      "metadata": {
        "id": "aLH39j5yhjVv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}